{
  "files": [
    {
      "path": "src/__init__.py",
      "language": "python",
      "description": "Initialization script for the src package that enables package-level imports and configurations.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "src/data_loader.py",
      "language": "python",
      "description": "This file defines a class `QuranDataLoader` to load Quran data from a text file. It includes functionalities to parse each line of the file, tokenize the verse text, and extract root words and lemmas using camel_tools. It also defines constants for MAKKI and MADANI surahs lists.",
      "classes": [
        {
          "name": "QuranDataLoader",
          "description": "A class to load Quran data from a text file.\n\nThe text file is expected to have each line in the format:\nsurah|ayah|verse_text",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initialize the data loader.\n\n:param file_path: Path to the Quran data file. If None, defaults to a predefined path.",
              "parameters": [
                {
                  "name": "file_path",
                  "type": "str",
                  "description": "Path to the Quran data file."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "load_data",
              "description": "Load and parse the Quran data file.\n\n:return: List of dictionaries representing each verse with keys 'surah', 'ayah', 'verse_text', and 'roots'.",
              "parameters": [],
              "return_type": "list"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src.tokenizer",
        "camel_tools.morphology.database",
        "camel_tools.morphology.analyzer"
      ]
    },
    {
      "path": "src/text_preprocessor.py",
      "language": "python",
      "description": "Module defining the TextPreprocessor class which processes Arabic text. It performs normalization, tokenization, lemmatization, and root extraction, while logging each step for debugging and verification.",
      "classes": [
        {
          "name": "TextPreprocessor",
          "description": "A class for preprocessing Arabic text by normalizing, tokenizing, lemmatizing, and extracting root words. It logs various processing steps for debugging.",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initialize the TextPreprocessor and configure the logger.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "preprocess_text",
              "description": "Preprocess the Arabic text by performing normalization, tokenization, lemmatization, and root extraction. Returns the processed tokens joined by a space.",
              "parameters": [
                {
                  "name": "text",
                  "type": "string",
                  "description": "The input Arabic text."
                }
              ],
              "return_type": "string"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/arabic_normalization.py",
        "src/tokenizer.py",
        "src/lemmatizer.py",
        "src/root_extractor.py"
      ]
    },
    {
      "path": "src/logger_config.py",
      "language": "Python",
      "description": "This file configures and returns a logger for the Quran analysis application. It sets the logging level, file handler, and log message format to a structured JSON-like format for both machine parseability and human readability.",
      "classes": [],
      "functions": [
        {
          "name": "configure_logger",
          "description": "Configure and return a logger instance for the Quran analysis application. It sets up file logging with a specific JSON-like format and writes logs to 'quran_analysis.log'.",
          "parameters": [],
          "return_type": "Logger"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/main.py",
      "language": "Python",
      "description": "This is the main module of the Quran analysis application. It orchestrates the entire analysis process including data loading, text preprocessing, and various analyses such as word frequency, gematria distribution, text complexity, readability metrics, and comparative analysis between Makki and Madani Surahs. The module contains functions for generating summary reports and executing the main analysis workflow.",
      "classes": [],
      "functions": [
        {
          "name": "generate_summary",
          "description": "Generates a comprehensive summary of key analysis results. Aggregates metadata, word frequency analysis and gematria co-occurrence analysis.",
          "parameters": [
            {
              "name": "metadata",
              "type": "dict",
              "description": "Dictionary containing metadata such as tool_version, execution_time, input_file."
            },
            {
              "name": "unique_words_count",
              "type": "int",
              "description": "Total number of unique words."
            },
            {
              "name": "top_words",
              "type": "list",
              "description": "List of top frequent words (list of tuples)."
            },
            {
              "name": "gematria_cooccurrence",
              "type": "Counter",
              "description": "Counter object from gematria co-occurrence analysis."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_quran_text_complexity",
          "description": "Analyzes text complexity for the entire Quran. Loads the Quran data, concatenates the preprocessed text from all verses, calls the analyze_text_complexity() function, logs the resulting metrics, and returns the metrics.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_text_complexity",
          "description": "Analyzes text complexity for each Surah. Groups the verses by Surah, concatenates the preprocessed text for each Surah, calls the analyze_text_complexity() function for each Surah, and returns a dictionary mapping each Surah to its metrics.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_text_complexity",
          "description": "Analyzes text complexity for each Ayah. For each Ayah, preprocesses the verse text, calls the analyze_text_complexity() function, and returns a dictionary mapping each Ayah (formatted as \"surah|ayah\") to its metrics.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "main",
          "description": "Main function to orchestrate data loading, text preprocessing, and various analyses on the Quran text. Performs multiple analyses including word frequency, gematria distribution, text complexity, readability metrics, and comparative analysis between Makki and Madani Surahs. Generates a comprehensive summary of results.",
          "parameters": [],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "logging",
        "os",
        "json",
        "datetime",
        "collections.Counter",
        "src.logger_config",
        "src.data_loader",
        "src.text_preprocessor",
        "src.text_complexity_analyzer",
        "src.gematria_analyzer",
        "src.frequency_analyzer",
        "src.correlation_analyzer",
        "src.distribution_analyzer",
        "src.cooccurrence_analyzer",
        "src.collocation_analyzer",
        "src.semantic_analyzer",
        "src.ngram_analyzer",
        "src.anomaly_detector",
        "src.readability_analyzer",
        "src.comparative_analyzer"
      ]
    },
    {
      "path": "setup.py",
      "language": "python",
      "description": "Setup script for packaging the QuranAnalysis application using setuptools. It defines package information, dependencies, and console script entry points.",
      "classes": [],
      "functions": [],
      "dependencies": [
        "setuptools"
      ]
    },
    {
      "path": "requirements.txt",
      "language": "None",
      "description": "Project dependency requirements including setuptools and numpy.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "data/quran-uthmani-min.txt",
      "language": "None",
      "description": "Data file containing the Quran text in a simple format with surah|ayah|verse text.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/__init__.py",
      "language": "python",
      "description": "Initialization script for the tests package that enables discovery and execution of test cases.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/test_data_loader.py",
      "language": "Python",
      "description": "This file contains unit tests for the QuranDataLoader class. It tests the functionality of loading data from a file, including error handling for file not found scenarios. The tests use a temporary file with test data to verify that the loader correctly parses the file and returns the expected data structure.",
      "classes": [
        {
          "name": "TestQuranDataLoader",
          "description": "A test class for the QuranDataLoader that verifies its functionality for loading and parsing Quran data from a text file. It includes tests for both successful data loading and error handling when a file is not found.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "setUp",
              "description": "Sets up the test environment by creating a temporary file with test data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "tearDown",
              "description": "Cleans up the test environment by removing the temporary file.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_load_data_file_not_found",
              "description": "Tests that load_data raises FileNotFoundError when the file doesn't exist.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_load_data_success",
              "description": "Tests that load_data correctly parses the file and returns the expected data structure.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "unittest",
        "os",
        "tempfile",
        "unittest.mock",
        "src.data_loader"
      ]
    },
    {
      "path": "tests/test_text_preprocessor.py",
      "language": "python",
      "description": "This file contains unit tests for the text preprocessing components including the TextPreprocessor class, the normalize_text function, and the tokenize_text function. It verifies that diacritics are removed, texts are properly normalized, and tokenization is executed correctly.",
      "classes": [
        {
          "name": "TestTextPreprocessor",
          "description": "Unit tests for text preprocessing functionalities. Tests cover normalization of Arabic text and proper splitting of text tokens.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_preprocess_text_removes_diacritics_and_normalizes",
              "description": "Tests that the TextPreprocessor correctly removes diacritics and normalizes Arabic letters in the input text.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_preprocess_text_no_modification",
              "description": "Tests that the TextPreprocessor leaves already normalized text unchanged.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_arabic_normalization_removes_invisible_and_normalizes",
              "description": "Tests that the normalize_text function removes invisible characters and diacritics while normalizing Arabic letters according to the mapping rules.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_tokenizer_splits_on_punctuation_and_whitespace",
              "description": "Tests that the tokenize_text function correctly splits input text into tokens based on punctuation and whitespace.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/text_preprocessor.py",
        "src/arabic_normalization.py",
        "src/tokenizer.py"
      ]
    },
    {
      "path": "tests/test_logger_config.py",
      "language": "python",
      "description": "Unit tests for the logger configuration ensuring the logger is set up with a FileHandler for the log file.",
      "classes": [
        {
          "name": "TestLoggerConfig",
          "description": "Unit tests for testing logger configuration via the configure_logger function.",
          "parents": [],
          "methods": [
            {
              "name": "test_configure_logger_creates_log_file",
              "description": "Test that the logger is configured with a FileHandler pointing to 'quran_analysis.log'.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/logger_config.py"
      ]
    },
    {
      "path": "tests/test_integration.py",
      "language": "python",
      "description": "This test module contains integration tests for the core user flows of the application. It validates data loading, logging, and the integration of various analytical functions by using unittest to simulate full execution paths.",
      "classes": [
        {
          "name": "TestIntegration",
          "description": "Integration tests for the core user flow including data processing, analysis result verification, and logging validation.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_integration_flow",
              "description": "Tests the complete integration flow of the application, verifying logging messages, data processing, and analysis outputs through the main function.",
              "parameters": [
                {
                  "name": "self",
                  "type": "string",
                  "description": "Instance of TestIntegration."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_semantic_group_cooccurrence_analysis",
              "description": "Tests the semantic group co-occurrence analysis function with sample data and verifies the correctness of its output and log messages.",
              "parameters": [
                {
                  "name": "self",
                  "type": "string",
                  "description": "Instance of TestIntegration."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_semantic_group_gematria_distribution",
              "description": "Tests the semantic group Gematria distribution analysis using sample data to ensure that the computed gematria values match the expected results.",
              "parameters": [
                {
                  "name": "self",
                  "type": "string",
                  "description": "Instance of TestIntegration."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "os",
        "re",
        "unittest",
        "src/main.py",
        "src/semantic_analyzer.py",
        "src/gematria_analyzer.py"
      ]
    },
    {
      "path": "src/arabic_normalization.py",
      "language": "python",
      "description": "This file implements a function 'normalize_text' that performs comprehensive Arabic text normalization. It removes invisible Unicode characters, strips Arabic diacritics, and maps various Arabic letter forms to their standard forms. Special handling is provided to convert taa marbuta to ha when it follows a ya.",
      "classes": [],
      "functions": [
        {
          "name": "normalize_text",
          "description": "Normalizes Arabic text by removing invisible Unicode characters and diacritics, applying a mapping to standardize Arabic letters, and converting taa marbuta to ha when preceded by ya. The function uses regular expressions for pattern matching and substitution.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input Arabic text that needs normalization."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/tokenizer.py",
      "language": "python",
      "description": "Module to tokenize Arabic text into individual word tokens using whitespace and punctuation as delimiters.",
      "classes": [],
      "functions": [
        {
          "name": "tokenize_text",
          "description": "Tokenize the normalized Arabic text into a list of word tokens based on whitespace and punctuation.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input normalized Arabic text to be tokenized."
            }
          ],
          "return_type": "list"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/lemmatizer.py",
      "language": "python",
      "description": "Module for Arabic lemmatization. Provides functionality to lemmatize Arabic tokens using CAMeL Tools.",
      "classes": [],
      "functions": [
        {
          "name": "lemmatize_token",
          "description": "Lemmatize the given Arabic token using CAMeL Tools. Returns the lemmatized form if successful; otherwise returns the original token.",
          "parameters": [
            {
              "name": "token",
              "type": "string",
              "description": "The Arabic word token."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/root_extractor.py",
      "language": "python",
      "description": "Module for Arabic root word extraction. Provides functionality to extract the root of an Arabic token using CAMeL Tools morphological analysis.",
      "classes": [],
      "functions": [
        {
          "name": "extract_root",
          "description": "Extract the root of the given Arabic token using CAMeL Tools morphological analysis. Returns the root if found; otherwise returns the original token.",
          "parameters": [
            {
              "name": "token",
              "type": "string",
              "description": "The Arabic word token."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "tests/test_lemmatizer.py",
      "language": "Python",
      "description": "Unit tests for the Arabic lemmatizer function. Uses unittest and mock patching to validate the behavior of lemmatize_token by simulating the lemmatizer instance.",
      "classes": [
        {
          "name": "TestLemmatizer",
          "description": "Contains unit tests for the Arabic lemmatizer. Tests the lemmatize_token function to ensure it appends '_lem' to a given token using a mocked lemmatizer instance.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_lemmatize_token",
              "description": "Tests that lemmatize_token returns the token appended with '_lem' using a mocked lemmatizer instance.",
              "parameters": [
                {
                  "name": "mock_lemmatizer_instance",
                  "type": "Mock",
                  "description": "A mocked instance of the internal lemmatizer to simulate the lemmatization behavior."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/lemmatizer"
      ]
    },
    {
      "path": "tests/test_root_extractor.py",
      "language": "Python",
      "description": "Unit tests for the Arabic root extraction functionality. Validates the behavior of extract_root function with both valid analysis and fallback when analysis returns an empty result.",
      "classes": [
        {
          "name": "TestRootExtractor",
          "description": "Contains unit tests for Arabic root extraction, testing both cases when a valid root is returned and when no analysis result is provided.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_extract_root",
              "description": "Tests that extract_root returns the correct root when the analyzer returns a valid result.",
              "parameters": [
                {
                  "name": "mock_analyzer_instance",
                  "type": "Mock",
                  "description": "A mocked analyzer instance to simulate analysis returning a valid root."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_extract_root_no_analysis",
              "description": "Tests that extract_root returns the original token when the analyzer returns an empty result.",
              "parameters": [
                {
                  "name": "mock_analyzer_instance",
                  "type": "Mock",
                  "description": "A mocked analyzer instance to simulate analysis returning an empty list."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/root_extractor"
      ]
    },
    {
      "path": "src/frequency_analyzer.py",
      "language": "Python",
      "description": "This module provides a range of analysis functions for Quran data including word frequency counts, word length distributions, root and lemma frequency analyses, semantic group frequency, character frequency, and sentence length distributions at both surah and ayah levels. It leverages logging to output detailed statistical information.",
      "classes": [],
      "functions": [
        {
          "name": "count_word_frequencies",
          "description": "Counts the frequency of each word in the tokenized text by iterating through lists of tokens.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists, where each inner list contains words from a verse."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_word_length_distribution",
          "description": "Analyzes the distribution of word lengths in the tokenized text, logging total words analyzed, average word length, and the most frequent word lengths.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists containing tokenized words."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_word_frequency",
          "description": "Analyzes word frequencies at the Surah level by updating counters for each surah and logging the top 10 most frequent words.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_word_frequency",
          "description": "Analyzes word frequencies at the Ayah level by mapping each (Surah, Ayah) tuple to a frequency counter and logging the top 5 words.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_root_word_frequency",
          "description": "Analyzes the frequency of root words across Quran data, logging total unique root words and the top 1000 most frequent root words.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with a 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_lemma_word_frequency",
          "description": "Analyzes the frequency of lemma words in Quran data, logging total unique lemma words and the top 1000 most frequent lemma words.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with a 'lemmas' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_root_word_frequency",
          "description": "Analyzes the frequency of root words at the Surah level using a text preprocessor, updating counters and logging the top 10 root words and the unique count.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_root_word_frequency",
          "description": "Analyzes the frequency of root words at the Ayah level by tokenizing text and extracting roots, then logging the frequency distribution per ayah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_first_root_word_frequency",
          "description": "Analyzes the frequency of the first root word in each ayah and logs the top 10 most frequent first root words along with their counts.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with a 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_last_root_word_frequency",
          "description": "Analyzes the frequency of the last root word in each ayah and logs the top 10 most frequent last root words along with their counts.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with a 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_semantic_group_frequency",
          "description": "Analyzes the frequency of semantic groups defined by root words in Quran data, logging the total unique semantic groups and the top 20 most frequent groups.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data, each containing a 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_character_frequency",
          "description": "Analyzes character frequency in preprocessed Quran text by iterating over ayahs and words, logging the top 20 most frequent characters and the total count of unique characters.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of ayahs, where each ayah is a list of words."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_character_frequency",
          "description": "Analyzes character frequency at the Surah level by concatenating verses and computing character counts, then logging the details.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_character_frequency",
          "description": "Analyzes character frequency at the Ayah level, logging character counts for each ayah identified by a unique Surah and Ayah combination.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_sentence_length_distribution",
          "description": "Analyzes the distribution of sentence lengths across the entire Quran by treating each ayah as a sentence and logging the frequency distribution.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists where each inner list represents tokenized words of an ayah."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_sentence_length_distribution",
          "description": "Analyzes sentence length distribution at the Surah level by tokenizing processed text from ayahs and computing frequency distributions.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with processed text."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_sentence_length_distribution",
          "description": "Analyzes sentence length for each ayah by counting the number of words in the processed text and mapping ayah identifiers to their sentence length frequency.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with ayah identifiers and processed text."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/text_preprocessor.py",
        "src/tokenizer.py",
        "src/root_extractor.py"
      ]
    },
    {
      "path": "src/cooccurrence_analyzer.py",
      "language": "python",
      "description": "This module provides functionality to analyze word co-occurrence within each ayah of the Quran data. It tokenizes the text and counts unique co-occurring word pairs, logging the top N pairs and the total number of unique pairs.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_cooccurrence",
          "description": "Analyzes word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_root_word_cooccurrence",
          "description": "Analyzes Root word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique root word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_lemma_word_cooccurrence",
          "description": "Analyzes Lemma word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique lemma word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/tokenizer.py"
      ]
    },
    {
      "path": "src/ngram_analyzer.py",
      "language": "Python",
      "description": "This Python module provides functions for analyzing both word and character n-grams in Quran data at multiple levels (Quran, Surah, and Ayah). It uses sliding window techniques and logging to output frequency analyses.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_ngrams",
          "description": "Analyzes word n-gram frequency for the Quran data by tokenizing each ayah, generating n-grams using a sliding window approach, logging the top frequent n-grams, and returning a Counter object of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of ayahs where each ayah can be a pre-tokenized list or a string that needs to be split."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Size of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "Counter"
        },
        {
          "name": "analyze_surah_word_ngrams",
          "description": "Analyzes word n-gram frequency at the Surah level by grouping Quran data by surah, consolidating tokens, generating n-grams, logging the top 10 frequent n-grams, and returning a dictionary mapping each surah to its Counter of n-grams.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys such as 'surah', 'ayah', 'verse_text', and optionally 'processed_text' and 'surah_name'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Size of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_word_ngrams",
          "description": "Analyzes word n-gram frequency at the Ayah level by processing each ayah, generating n-grams, logging the top 5 n-grams, and returning a dictionary mapping each 'Surah|Ayah' identifier to a Counter object of n-grams.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys like 'surah', 'ayah', 'verse_text', and optionally 'processed_text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Size of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_character_ngrams",
          "description": "Analyzes character n-gram frequency for the entire Quran text by concatenating ayahs, generating character n-grams with a sliding window, logging the top 10 n-grams, and returning a Counter object of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys such as 'processed_text', 'verse_text', or 'text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Length of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "Counter"
        },
        {
          "name": "analyze_surah_character_ngrams",
          "description": "Analyzes character n-grams at the Surah level by grouping Quran data by surah, concatenating texts, generating character n-grams, logging the top 10 n-grams for each surah, and returning a dictionary mapping each surah to its Counter of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys such as 'processed_text', 'verse_text', or 'text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Length of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_character_ngrams",
          "description": "Analyzes character n-grams at the Ayah level for a sample of ayahs (first 5 per surah) by generating n-grams, logging the top 5 n-grams, and returning a dictionary mapping each 'Surah|Ayah' identifier to a Counter object of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys like 'surah', 'ayah', and various text fields such as 'processed_text', 'text', or 'verse_text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Length of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": []
    },
    {
      "path": "tests/test_ngram_analyzer.py",
      "language": "Python",
      "description": "This file contains unit tests for the ngram_analyzer module. It uses Python's unittest framework to validate character n-gram analysis functions at various levels (Quran, Surah, Ayah) and for different n-gram sizes.",
      "classes": [
        {
          "name": "TestCharacterNGrams",
          "description": "Unit tests for verifying character n-gram analysis functions from the ngram_analyzer module. It includes tests for n=2, n=3, and surah/ayah level analyses.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_analyze_character_ngrams_n2",
              "description": "Tests analyze_character_ngrams with n=2 on sample data to ensure correct frequency counts of character bigrams.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_analyze_character_ngrams_n3",
              "description": "Tests analyze_character_ngrams with n=3 on sample data to validate frequency counts of character trigrams.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_analyze_surah_character_ngrams",
              "description": "Tests analyze_surah_character_ngrams by providing sample data and comparing expected Counter outputs for surah-level character n-gram analysis.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_analyze_ayah_character_ngrams",
              "description": "Tests analyze_ayah_character_ngrams to ensure that ayah-level grouping and character n-gram frequency counts match expected results.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/ngram_analyzer.py"
      ]
    },
    {
      "path": "src/collocation_analyzer.py",
      "language": "python",
      "description": "Module for analyzing word collocations in Quran text. Contains a function `analyze_word_collocation` to find and count word pairs within a specified window size in Quranic verses. It preprocesses text, tokenizes it, and uses a sliding window to identify and count collocations, logging the analysis details.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_collocation",
          "description": "Analyze word collocations in the given Quran data using a sliding window.\n    \n    For each ayah in quran_data, the function preprocesses the verse text \n    (using the existing TextPreprocessor if the processed text is not already present)\n    and tokenizes the text into words. For each word, it considers a window of adjacent \n    words (window_size to the left and window_size to the right, excluding the target word)\n    and counts each collocation pair. The collocation pairs are stored in alphabetical order \n    to ensure consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "A list of dictionaries representing ayahs. Each dictionary should contain at least a 'verse_text' key, and may contain a 'processed_text' key."
            },
            {
              "name": "window_size",
              "type": "int",
              "description": "The number of words to consider to the left and right of a target word. Default is 3."
            }
          ],
          "return_type": "Counter"
        }
      ],
      "dependencies": [
        "logging",
        "collections",
        "src.text_preprocessor"
      ]
    },
    {
      "path": "src/semantic_analyzer.py",
      "language": "python",
      "description": "Module for semantic analysis of Quran text. It includes functions to analyze semantic group co-occurrence within Quranic Ayahs by computing unique unordered pairs of semantic groups.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_semantic_group_cooccurrence_ayah",
          "description": "Analyzes semantic group co-occurrence in each Ayah by computing all unordered pairs of semantic groups from the 'semantic_groups' key, logging top pairs and total counts.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data, each expected to contain a 'semantic_groups' key."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/anomaly_detector.py",
      "language": "Python",
      "description": "This file contains functions for anomaly detection in frequency distributions. It computes z-scores for each entry in a distribution and logs anomalies when they exceed a specified threshold. This module is integral to analyzing Quranic feature data for deviations.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_single_distribution",
          "description": "Analyzes a single frequency distribution by computing z-scores and logging anomalies when counts deviate significantly from the mean.",
          "parameters": [
            {
              "name": "feature_name",
              "type": "string",
              "description": "Name of the feature to analyze."
            },
            {
              "name": "distribution",
              "type": "dict",
              "description": "Dictionary mapping feature items to their counts."
            },
            {
              "name": "context",
              "type": "string",
              "description": "Context specifying the level of analysis (e.g., 'Quran', 'Surah', 'Ayah')."
            },
            {
              "name": "threshold",
              "type": "float",
              "description": "Z-score threshold to consider a value anomalous (default is 2.0)."
            }
          ],
          "return_type": "None"
        },
        {
          "name": "analyze_anomaly_detection",
          "description": "Performs anomaly detection across multiple Quranic feature analyses by iterating over provided data and applying the z-score method at different hierarchical levels.",
          "parameters": [
            {
              "name": "analysis_results",
              "type": "dict",
              "description": "Dictionary containing various frequency analysis results for anomaly detection."
            }
          ],
          "return_type": "None"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/gematria_analyzer.py",
      "language": "python",
      "description": "This module provides a set of functions to calculate the Gematria values for Arabic words and analyze their distributions across Quranic text. It includes functions for computing gematria values with default or custom mappings, analyzing distributions at Surah and Ayah levels, and performing co-occurrence and semantic analyses.",
      "classes": [],
      "functions": [
        {
          "name": "calculate_gematria_value",
          "description": "Calculates the Gematria value of an Arabic word using a predefined mapping of Arabic letters to numeric values. Logs a warning if a character is not found in the mapping.",
          "parameters": [
            {
              "name": "word",
              "type": "string",
              "description": "Arabic word whose Gematria value is to be calculated."
            }
          ],
          "return_type": "int"
        },
        {
          "name": "analyze_gematria_value_distribution",
          "description": "Analyzes the frequency distribution of Gematria values across tokenized Quran text by summing the values of each word and counting frequencies.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "A list of lists containing tokenized Arabic words from the Quran."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "get_default_gematria_mapping",
          "description": "Returns the default mapping of Arabic letters to their Gematria numerical values.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "calculate_gematria_value_with_mapping",
          "description": "Calculates the Gematria value of a given word using a provided mapping. Logs a warning for characters not found in the supplied mapping.",
          "parameters": [
            {
              "name": "word",
              "type": "string",
              "description": "Arabic word whose Gematria value is to be calculated."
            },
            {
              "name": "gematria_mapping",
              "type": "dict",
              "description": "Mapping of Arabic letters to their numeric Gematria values."
            }
          ],
          "return_type": "int"
        },
        {
          "name": "analyze_surah_gematria_distribution",
          "description": "Analyzes Gematria value distribution at the Surah level from Quran data. Calculates frequency and summary statistics for each Surah.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            },
            {
              "name": "gematria_mapping",
              "type": "dict",
              "description": "Mapping of Arabic letters to numeric Gematria values."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_gematria_distribution",
          "description": "Analyzes Gematria value distribution at the Ayah level from Quran data. Returns frequency counts and summary statistics for each Ayah.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries containing Quran verse data."
            },
            {
              "name": "gematria_mapping",
              "type": "dict",
              "description": "Mapping of Arabic letters to numeric Gematria values."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_first_word_gematria_ayah",
          "description": "Analyzes the Gematria value for the first word in each Ayah and returns frequency counts.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            },
            {
              "name": "gematria_mapping",
              "type": "dict",
              "description": "Mapping of Arabic letters to numeric Gematria values."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_last_word_gematria_ayah",
          "description": "Analyzes the Gematria value for the last word in each Ayah and returns frequency counts.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            },
            {
              "name": "gematria_mapping",
              "type": "dict",
              "description": "Mapping of Arabic letters to numeric Gematria values."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_gematria_cooccurrence_ayah",
          "description": "Analyzes the co-occurrence of Gematria values within each Ayah by identifying unordered pairs and counting their frequency across the Quran data.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran verse data."
            }
          ],
          "return_type": "Counter"
        },
        {
          "name": "analyze_semantic_group_gematria_distribution",
          "description": "Analyzes the distribution of Gematria values for words belonging to each semantic group in the Quran data.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with semantic group information."
            },
            {
              "name": "gematria_mapping",
              "type": "dict",
              "description": "Mapping of Arabic letters to their Gematria values."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_gematria_distribution_by_sentence_length",
          "description": "Analyzes the distribution of Gematria values of words grouped by sentence length from Quran data.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries containing Quran verse data."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "logging",
        "statistics",
        "itertools",
        "collections",
        "src/main.py",
        "tests/test_integration.py"
      ]
    },
    {
      "path": "tests/test_quran_analysis.py",
      "language": "Python",
      "description": "Unit tests for Gematria analyzer and readability analysis functions. Contains tests validating gematria distributions and readability score calculations across multiple scenarios.",
      "classes": [
        {
          "name": "TestGematriaAnalyzer",
          "description": "Unit tests for the Gematria analyzer functions at various levels.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_analyze_surah_gematria_distribution",
              "description": "Tests the analysis of Surah gematria distribution using sample data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_ayah_gematria_distribution",
              "description": "Tests the analysis of Ayah gematria distribution using sample data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_first_word_gematria_ayah",
              "description": "Tests gematria analysis on the first word of each Ayah using sample data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_last_word_gematria_ayah",
              "description": "Tests gematria analysis on the last word of each Ayah using sample data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_dale_chall_readability_empty",
              "description": "Tests the Dale-Chall Readability Score calculation for empty text input.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_dale_chall_readability_non_empty",
              "description": "Tests the Dale-Chall Readability Score calculation for non-empty text input with common words.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_smog_index_empty",
              "description": "Tests the SMOG Index calculation for empty text input.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_smog_index_non_empty",
              "description": "Tests the SMOG Index calculation for non-empty text input.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_dale_chall_readability_multiple_sentences",
              "description": "Tests the Dale-Chall Readability Score calculation with multiple sentences.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_smog_index_multiple_sentences",
              "description": "Tests the SMOG Index calculation with multiple sentences.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaAnalyzer",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/gematria_analyzer.py",
        "src/readability_analyzer.py"
      ]
    },
    {
      "path": "tests/test_gematria_analyzer.py",
      "language": "Python",
      "description": "Unit tests for the Gematria analyzer module. This file focuses on testing the analyze_gematria_cooccurrence_ayah function across various scenarios.",
      "classes": [
        {
          "name": "TestGematriaCooccurrenceAnalysis",
          "description": "Unit tests for the analyze_gematria_cooccurrence_ayah function, ensuring it handles different input cases correctly.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_empty_text",
              "description": "Tests that the function returns an empty Counter when provided with empty processed text.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaCooccurrenceAnalysis",
                  "description": "Instance of TestGematriaCooccurrenceAnalysis."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_single_word",
              "description": "Tests that a single word input produces no gematria pairs.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaCooccurrenceAnalysis",
                  "description": "Instance of TestGematriaCooccurrenceAnalysis."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_unique_values",
              "description": "Tests that unique input words generate expected unique gematria pairs.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaCooccurrenceAnalysis",
                  "description": "Instance of TestGematriaCooccurrenceAnalysis."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_repeated_values",
              "description": "Tests that repeated values in input are counted correctly in gematria pair combinations.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaCooccurrenceAnalysis",
                  "description": "Instance of TestGematriaCooccurrenceAnalysis."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_multiple_ayahs",
              "description": "Tests the function with multiple ayahs to ensure correct aggregation of gematria pairs.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestGematriaCooccurrenceAnalysis",
                  "description": "Instance of TestGematriaCooccurrenceAnalysis."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/gematria_analyzer.py"
      ]
    },
    {
      "path": "tests/test_sentence_length_distribution.py",
      "language": "Python",
      "description": "This module contains integration tests for sentence length distribution analysis functions from both distribution_analyzer and frequency_analyzer modules, using the unittest framework to validate statistical computations and logging outputs.",
      "classes": [
        {
          "name": "TestSentenceLengthDistribution",
          "description": "Integration tests for sentence length distribution analysis functions in both distribution_analyzer and frequency_analyzer modules.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_analyze_sentence_length_distribution",
              "description": "Tests the analyze_sentence_length_distribution function with sample tokenized text.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestSentenceLengthDistribution",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_surah_sentence_length_distribution",
              "description": "Tests the analyze_surah_sentence_length_distribution function with sample Quran data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestSentenceLengthDistribution",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_ayah_sentence_length_distribution",
              "description": "Tests the analyze_ayah_sentence_length_distribution function using sample Quran data with surah and ayah identifiers.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestSentenceLengthDistribution",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_surah_sentence_length_distribution_by_index",
              "description": "Tests the analyze_surah_sentence_length_distribution_by_index function with sample data containing surah numbers.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestSentenceLengthDistribution",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_ayah_sentence_length_distribution_by_index",
              "description": "Tests the analyze_ayah_sentence_length_distribution_by_index function with sample data containing ayah indices.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestSentenceLengthDistribution",
                  "description": "Instance of the test case."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/frequency_analyzer.py",
        "src/distribution_analyzer.py"
      ]
    },
    {
      "path": "src/distribution_analyzer.py",
      "language": "Python",
      "description": "This module contains functions to analyze sentence length distribution by surah and ayah index. It calculates frequency distributions, averages, medians, modes, and standard deviations of sentence lengths from Quran data and logs the summary statistics for further analysis.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_surah_sentence_length_distribution_by_index",
          "description": "Analyzes sentence length distribution for each Surah index by extracting ayah lengths, computing frequency, average, median, mode, and standard deviation, and logging the results.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_sentence_length_distribution_by_index",
          "description": "Analyzes sentence length distribution for each Ayah index across all surahs by computing frequency distribution, average, median, mode, and standard deviation from Quran data, and logs the output.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/text_complexity_analyzer.py",
      "language": "Python",
      "description": "This file provides functionality for analyzing text complexity metrics for Arabic text, particularly focused on the Quran. It includes functions to calculate various readability metrics such as average word length, average sentence length, Flesch Reading Ease score, and Flesch-Kincaid Grade Level. The file also contains functions to analyze these metrics at different levels of granularity: for the entire Quran, for each Surah, and for each Ayah.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_text_complexity",
          "description": "Analyzes basic text complexity metrics for a preprocessed Arabic text by calculating average word length and average sentence length. Sentences are assumed to be separated by newline characters.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "Preprocessed Arabic text as a string."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "calculate_flesch_reading_ease",
          "description": "Calculates the Flesch Reading Ease score for the given preprocessed text using the formula: 206.835 - 1.015*(total words/total sentences) - 84.6*(total syllables/total words). Syllables are approximated as the count of vowels in each word.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "Preprocessed text as a string."
            }
          ],
          "return_type": "float"
        },
        {
          "name": "calculate_flesch_kincaid_grade_level",
          "description": "Calculates the Flesch-Kincaid Grade Level for the given preprocessed text using the formula: 0.39*(total words/total sentences) + 11.8*(total syllables/total words) - 14.59. Syllables are approximated as the count of vowels in each word.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "Preprocessed text as a string."
            }
          ],
          "return_type": "float"
        },
        {
          "name": "analyze_quran_flesch_reading_ease",
          "description": "Analyzes and logs the Flesch Reading Ease score for the entire Quran by loading the Quran data, preprocessing the text, and calculating the score.",
          "parameters": [],
          "return_type": "float"
        },
        {
          "name": "analyze_quran_flesch_kincaid_grade_level",
          "description": "Analyzes and logs the Flesch-Kincaid Grade Level for the entire Quran by loading the Quran data, preprocessing the text, and calculating the grade level.",
          "parameters": [],
          "return_type": "float"
        },
        {
          "name": "analyze_surah_flesch_reading_ease",
          "description": "Analyzes and logs the Flesch Reading Ease score for each Surah by grouping the Quran data by Surah and calculating the score for each group.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_flesch_kincaid_grade_level",
          "description": "Analyzes and logs the Flesch-Kincaid Grade Level for each Surah by grouping the Quran data by Surah and calculating the grade level for each group.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_flesch_reading_ease",
          "description": "Analyzes and logs the Flesch Reading Ease score for each Ayah by processing each Ayah individually and calculating its score.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_flesch_kincaid_grade_level",
          "description": "Analyzes and logs the Flesch-Kincaid Grade Level for each Ayah by processing each Ayah individually and calculating its grade level.",
          "parameters": [],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/logger_config.py",
        "src/data_loader.py",
        "src/text_preprocessor.py"
      ]
    },
    {
      "path": "src/correlation_analyzer.py",
      "language": "Python",
      "description": "This module performs correlation analysis between sentence length and average Gematria value in Quranic text data. It defines the function analyze_sentence_length_gematria_correlation which processes a list of verse dictionaries, tokenizes text, computes Gematria values, groups results by sentence length, calculates average Gematria per group, and computes the Pearson correlation coefficient between sentence lengths and Gematria values.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_sentence_length_gematria_correlation",
          "description": "Analyzes the correlation between the number of words in an ayah and the average Gematria value of its words. It processes each verse by tokenizing text, computing individual Gematria values using an external gematria function, grouping results by sentence length, and finally calculating both group averages and the Pearson correlation coefficient.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing preprocessed Quran data."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/gematria_analyzer.py"
      ]
    },
    {
      "path": "tests/test_correlation_analyzer.py",
      "language": "Python",
      "description": "This module contains unit tests for the Sentence Length vs Gematria Correlation Analysis. It uses the unittest framework to test the functionality of analyze_sentence_length_gematria_correlation, ensuring correct computation of group averages and Pearson correlation coefficients with sample data.",
      "classes": [
        {
          "name": "TestCorrelationAnalyzer",
          "description": "Unit test class for testing the analyze_sentence_length_gematria_correlation function.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_analyze_sentence_length_gematria_correlation",
              "description": "Tests the analyze_sentence_length_gematria_correlation function with sample data, verifying that the computed group averages and correlation coefficient match expected values.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/correlation_analyzer.py"
      ]
    },
    {
      "path": "src/semantic_distribution_analyzer.py",
      "language": "Python",
      "description": "This file contains a function to analyze semantic complexity distribution of Quran Ayahs. It calculates semantic density by counting occurrences of root words, computes text complexity metrics using an external analyzer, groups ayahs based on quantile thresholds, and logs comprehensive statistics. It also employs a fallback strategy for cases with insufficient variability in semantic density.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_semantic_complexity_distribution_ayah",
          "description": "Analyzes semantic complexity distribution by calculating semantic density for each Ayah in the provided Quran data and computing text complexity metrics. It groups Ayahs into low, medium, and high categories based on quantile thresholds and computes descriptive statistics for each group. A fallback strategy is used if quantile calculation fails or variability is insufficient.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data. Each dictionary should include keys: 'verse_text', 'processed_text', 'roots', 'surah', and 'ayah'."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/text_complexity_analyzer.py"
      ]
    },
    {
      "path": "tests/test_semantic_distribution_analyzer.py",
      "language": "Python",
      "description": "This file contains unit tests for the analyze_semantic_complexity_distribution_ayah function. It verifies normal execution with sufficient variability in semantic density as well as fallback scenarios where all ayahs have the same semantic density, ensuring correct quantile boundary calculation and grouping statistics.",
      "classes": [
        {
          "name": "TestSemanticDistributionAnalyzer",
          "description": "Integration tests for the analyze_semantic_complexity_distribution_ayah function covering both normal and fallback scenarios.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_analyze_semantic_complexity_distribution_ayah",
              "description": "Tests normal execution of analyze_semantic_complexity_distribution_ayah using sample Quran data with sufficient variability. Verifies that quantile boundaries exist and group statistics for low, medium, and high groups are correctly computed.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_analyze_semantic_complexity_distribution_ayah_with_insufficient_variability",
              "description": "Tests the fallback scenario of analyze_semantic_complexity_distribution_ayah where all ayahs have the same semantic density. Verifies that quantile boundaries are None and that all ayahs are assigned to the 'medium' group.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/semantic_distribution_analyzer.py"
      ]
    },
    {
      "path": "tests/test_text_complexity_enhancement.py",
      "language": "python",
      "description": "This test module contains unit and integration tests for verifying the functionality of text complexity enhancement functions. It tests computations for Flesch Reading Ease and Flesch-Kincaid Grade Level at Quran, Surah, and Ayah levels.",
      "classes": [
        {
          "name": "TestFleschMetrics",
          "description": "Unit tests for Flesch Reading Ease and Flesch-Kincaid Grade Level calculations.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_calculate_flesch_reading_ease_empty",
              "description": "Tests that calculate_flesch_reading_ease returns 0.0 when provided with an empty string.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestFleschMetrics",
                  "description": "Instance of TestFleschMetrics"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_flesch_kincaid_grade_level_empty",
              "description": "Tests that calculate_flesch_kincaid_grade_level returns 0.0 when provided with an empty string.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestFleschMetrics",
                  "description": "Instance of TestFleschMetrics"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_flesch_reading_ease_known",
              "description": "Tests calculate_flesch_reading_ease with a known input to verify correct score computation.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestFleschMetrics",
                  "description": "Instance of TestFleschMetrics"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_calculate_flesch_kincaid_grade_level_known",
              "description": "Tests calculate_flesch_kincaid_grade_level with a known input to verify correct grade level computation.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestFleschMetrics",
                  "description": "Instance of TestFleschMetrics"
                }
              ],
              "return_type": "None"
            }
          ]
        },
        {
          "name": "TestFleschIntegrationAnalysis",
          "description": "Integration tests for Quran, Surah, and Ayah level Flesch analyses.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_integration_quran_level",
              "description": "Tests the integration of Quran level Flesch analysis functions by verifying numeric outputs.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestFleschIntegrationAnalysis",
                  "description": "Instance of TestFleschIntegrationAnalysis"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_integration_surah_and_ayah_level",
              "description": "Tests the integration of Surah and Ayah level Flesch analysis functions by verifying dictionary outputs with expected keys.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestFleschIntegrationAnalysis",
                  "description": "Instance of TestFleschIntegrationAnalysis"
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/text_complexity_analyzer.py",
        "os",
        "unittest"
      ]
    },
    {
      "path": "src/readability_analyzer.py",
      "language": "Python",
      "description": "This module provides advanced readability analysis for Arabic text, implementing two readability metrics: the Dale-Chall Readability Score (adapted for Arabic) and the SMOG Index. It includes functions to analyze these metrics at three levels: entire Quran text, per Surah, and per Ayah. The module also provides utility functions for text processing and readability calculations.",
      "classes": [],
      "functions": [
        {
          "name": "load_common_arabic_words",
          "description": "Loads a comprehensive set of common Arabic words from a file if available. Checks the environment variable 'COMMON_WORDS_FILE'. If the file exists, loads words from it. Otherwise, returns a default set of common words.",
          "parameters": [],
          "return_type": "set"
        },
        {
          "name": "split_sentences",
          "description": "Splits the given text into sentences based on the specified delimiter.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "The text to split."
            },
            {
              "name": "delimiter",
              "type": "str",
              "description": "The delimiter to use for splitting (default is newline)."
            }
          ],
          "return_type": "list"
        },
        {
          "name": "is_polysyllabic",
          "description": "Determines if a word is polysyllabic based on its length. For Arabic text, this function uses a character length threshold to approximate syllable count.",
          "parameters": [
            {
              "name": "word",
              "type": "str",
              "description": "The word to check."
            },
            {
              "name": "threshold",
              "type": "int",
              "description": "The character length threshold above which a word is considered polysyllabic."
            }
          ],
          "return_type": "bool"
        },
        {
          "name": "calculate_dale_chall_readability",
          "description": "Calculates the Dale-Chall Readability Score for the given preprocessed Arabic text. The function tokenizes the text into words, counts difficult words, computes percentage of difficult words, and calculates average sentence length.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "Preprocessed Arabic text as a string."
            }
          ],
          "return_type": "float"
        },
        {
          "name": "calculate_smog_index",
          "description": "Calculates the SMOG Index for the given preprocessed Arabic text. For this approximation, words with more than the threshold characters are considered polysyllabic.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "Preprocessed Arabic text as a string."
            }
          ],
          "return_type": "float"
        },
        {
          "name": "analyze_quran_dale_chall_readability",
          "description": "Analyzes the Dale-Chall Readability Score for the entire Quran text. Loads the Quran data, preprocesses the text, concatenates all verses, and computes the score.",
          "parameters": [],
          "return_type": "float"
        },
        {
          "name": "analyze_surah_dale_chall_readability",
          "description": "Analyzes the Dale-Chall Readability Score for each Surah. Groups verses by Surah, preprocesses text, and computes scores for each Surah.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_dale_chall_readability",
          "description": "Analyzes the Dale-Chall Readability Score for each Ayah. Preprocesses each verse text and computes the score for each Ayah.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_quran_smog_index",
          "description": "Analyzes the SMOG Index for the entire Quran text. Loads the Quran data, preprocesses the text, concatenates all verses, and computes the SMOG Index.",
          "parameters": [],
          "return_type": "float"
        },
        {
          "name": "analyze_surah_smog_index",
          "description": "Analyzes the SMOG Index for each Surah. Groups verses by Surah, preprocesses text, and computes the SMOG Index for each Surah.",
          "parameters": [],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_smog_index",
          "description": "Analyzes the SMOG Index for each Ayah. Preprocesses each verse text and computes the SMOG Index for each Ayah.",
          "parameters": [],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "os",
        "math",
        "logging",
        "src.data_loader",
        "src.text_preprocessor"
      ]
    },
    {
      "path": "src/comparative_analyzer.py",
      "language": "python",
      "description": "Module for comparative analysis between Makki and Madani Surahs. Provides functions to compare text complexity, word frequency, and gematria distributions, along with computing approximate Dale-Chall and SMOG readability metrics.",
      "classes": [],
      "functions": [
        {
          "name": "compute_dale_chall",
          "description": "Compute an approximate Dale-Chall Readability Score for the given text using percentage of difficult words and average sentence length.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "Preprocessed text as a string."
            }
          ],
          "return_type": "float"
        },
        {
          "name": "compute_smog",
          "description": "Compute an approximate SMOG Index for the given text using an approximation based on polysyllabic words.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "Preprocessed text as a string."
            }
          ],
          "return_type": "float"
        },
        {
          "name": "compare_makki_madani_text_complexity",
          "description": "Compare text complexity metrics between Makki and Madani Surahs by filtering Quran data and computing metrics such as Flesch Reading Ease, Flesch-Kincaid Grade Level, Dale-Chall, and SMOG Index.",
          "parameters": [
            {
              "name": "makki_surahs",
              "type": "list",
              "description": "List of surah numbers classified as Makki."
            },
            {
              "name": "madani_surahs",
              "type": "list",
              "description": "List of surah numbers classified as Madani."
            }
          ],
          "return_type": "Dictionary"
        },
        {
          "name": "compare_makki_madani_word_frequency_distribution",
          "description": "Compare word frequency distributions between Makki and Madani Surahs and return the top frequent words for each group.",
          "parameters": [
            {
              "name": "makki_surahs",
              "type": "list",
              "description": "List of surah numbers classified as Makki."
            },
            {
              "name": "madani_surahs",
              "type": "list",
              "description": "List of surah numbers classified as Madani."
            },
            {
              "name": "top_n",
              "type": "int",
              "description": "Number of top frequent words to return."
            }
          ],
          "return_type": "Dictionary"
        },
        {
          "name": "compare_makki_madani_gematria_distribution",
          "description": "Compare Gematria value distributions between Makki and Madani Surahs by computing frequency distributions and returning the top frequent gematria values.",
          "parameters": [
            {
              "name": "makki_surahs",
              "type": "list",
              "description": "List of surah numbers classified as Makki."
            },
            {
              "name": "madani_surahs",
              "type": "list",
              "description": "List of surah numbers classified as Madani."
            },
            {
              "name": "top_n",
              "type": "int",
              "description": "Number of top gematria values to return."
            }
          ],
          "return_type": "Dictionary"
        }
      ],
      "dependencies": [
        "src/data_loader.py",
        "src/text_preprocessor.py",
        "src/text_complexity_analyzer.py",
        "src/frequency_analyzer.py",
        "src/gematria_analyzer.py"
      ]
    },
    {
      "path": "tests/integration_tests.py",
      "language": "Python",
      "description": "Integration tests for the comparative analysis functions. Sets up sample Quran data and validates outputs for text complexity, word frequency distribution, and Gematria distribution analyses.",
      "classes": [
        {
          "name": "TestComparativeAnalyzer",
          "description": "Integration tests for the comparative analysis functions, verifying text complexity, word frequency, and gematria distribution analyses using sample Quran data.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_compare_text_complexity",
              "description": "Tests the compare_makki_madani_text_complexity function by creating sample Quran data and verifying that metrics for both Makki and Madani groups are floats.",
              "parameters": [
                {
                  "name": "self",
                  "type": "object",
                  "description": "Instance of the test class."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_compare_word_frequency_distribution",
              "description": "Tests the compare_makki_madani_word_frequency_distribution function by verifying that the returned frequency lists for both Makki and Madani groups are non-empty.",
              "parameters": [
                {
                  "name": "self",
                  "type": "object",
                  "description": "Instance of the test class."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_compare_gematria_distribution",
              "description": "Tests the compare_makki_madani_gematria_distribution function by creating sample Quran data and checking that the output contains tuples of gematria values and frequencies.",
              "parameters": [
                {
                  "name": "self",
                  "type": "object",
                  "description": "Instance of the test class."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/comparative_analyzer.py",
        "src/data_loader.py"
      ]
    }
  ]
}