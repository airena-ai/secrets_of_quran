{
  "files": [
    {
      "path": "src/__init__.py",
      "language": "python",
      "description": "Initialization script for the src package that enables package-level imports and configurations.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "src/data_loader.py",
      "language": "Python",
      "description": "This file contains the QuranDataLoader class responsible for loading Quran verse data from a text file. It handles parsing lines in the format 'surah|ayah|verse_text' and converts them into a structured list of dictionaries. The class includes error handling for file not found scenarios and proper logging.",
      "classes": [
        {
          "name": "QuranDataLoader",
          "description": "A class that loads and parses Quran data from a text file. Each line in the file is expected to be in the format 'surah|ayah|verse_text'. The class provides functionality to read this data and convert it into a structured format for further processing.",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initializes the QuranDataLoader with an optional file path and sets up logging.",
              "parameters": [
                {
                  "name": "self",
                  "type": "QuranDataLoader",
                  "description": "Instance of the class"
                },
                {
                  "name": "file_path",
                  "type": "str",
                  "description": "Path to the data file. If None, returns empty data."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "load_data",
              "description": "Loads data from the specified file, parsing each line into a dictionary with 'surah', 'ayah', and 'verse_text' keys. Raises FileNotFoundError if the file path is not provided or the file does not exist.",
              "parameters": [
                {
                  "name": "self",
                  "type": "QuranDataLoader",
                  "description": "Instance of the class"
                }
              ],
              "return_type": "list"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "os",
        "logging"
      ]
    },
    {
      "path": "src/text_preprocessor.py",
      "language": "python",
      "description": "Module defining the TextPreprocessor class which processes Arabic text. It performs normalization, tokenization, lemmatization, and root extraction, while logging each step for debugging and verification.",
      "classes": [
        {
          "name": "TextPreprocessor",
          "description": "A class for preprocessing Arabic text by normalizing, tokenizing, lemmatizing, and extracting root words. It logs various processing steps for debugging.",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initialize the TextPreprocessor and configure the logger.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "preprocess_text",
              "description": "Preprocess the Arabic text by performing normalization, tokenization, lemmatization, and root extraction. Returns the processed tokens joined by a space.",
              "parameters": [
                {
                  "name": "text",
                  "type": "string",
                  "description": "The input Arabic text."
                }
              ],
              "return_type": "string"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/arabic_normalization.py",
        "src/tokenizer.py",
        "src/lemmatizer.py",
        "src/root_extractor.py"
      ]
    },
    {
      "path": "src/logger_config.py",
      "language": "python",
      "description": "Configures and returns a logger for the application, logging messages to both console and a log file named 'quran_analysis.log'.",
      "classes": [],
      "functions": [
        {
          "name": "configure_logger",
          "description": "Configures and returns a logger that logs messages to both console and a log file.\n\nThe log file is named 'quran_analysis.log' and is located in the project root.\n\n:return: Configured logger instance.",
          "parameters": [],
          "return_type": "logger"
        }
      ],
      "dependencies": [
        "logging",
        "os"
      ]
    },
    {
      "path": "src/main.py",
      "language": "python",
      "description": "This is the main entry point for the QuranAnalysis application. It orchestrates data loading, text preprocessing, and various frequency and statistical analyses on Quran data. It also manages logging, error handling, and sequential invocation of analysis modules.",
      "classes": [],
      "functions": [
        {
          "name": "main",
          "description": "Main function to orchestrate data loading, text preprocessing, frequency analysis, and various analytical computations on Quran data. It handles configuration, error logging, and coordinates multiple analysis tasks.",
          "parameters": [],
          "return_type": "None"
        }
      ],
      "dependencies": [
        "src/logger_config.py",
        "src/data_loader.py",
        "src/text_preprocessor.py",
        "src/frequency_analyzer.py",
        "src/cooccurrence_analyzer.py",
        "src/collocation_analyzer.py",
        "src/ngram_analyzer.py",
        "src/anomaly_detector.py",
        "src/semantic_analyzer.py",
        "src/tokenizer.py",
        "src/root_extractor.py"
      ]
    },
    {
      "path": "setup.py",
      "language": "python",
      "description": "Setup script for packaging the QuranAnalysis application using setuptools. It defines package information, dependencies, and console script entry points.",
      "classes": [],
      "functions": [],
      "dependencies": [
        "setuptools"
      ]
    },
    {
      "path": "requirements.txt",
      "language": "None",
      "description": "Requirements file listing the external dependencies required by the QuranAnalysis application.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "data/quran-uthmani-min.txt",
      "language": "None",
      "description": "Data file containing the Quran text in a simple format with surah|ayah|verse text.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/__init__.py",
      "language": "python",
      "description": "Initialization script for the tests package that enables discovery and execution of test cases.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/test_data_loader.py",
      "language": "Python",
      "description": "This file contains unit tests for the QuranDataLoader class. It tests the functionality of loading data from a file, including error handling for file not found scenarios. The tests use a temporary file with test data to verify that the loader correctly parses the file and returns the expected data structure.",
      "classes": [
        {
          "name": "TestQuranDataLoader",
          "description": "A test class for the QuranDataLoader that verifies its functionality for loading and parsing Quran data from a text file. It includes tests for both successful data loading and error handling when a file is not found.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "setUp",
              "description": "Sets up the test environment by creating a temporary file with test data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "tearDown",
              "description": "Cleans up the test environment by removing the temporary file.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_load_data_file_not_found",
              "description": "Tests that load_data raises FileNotFoundError when the file doesn't exist.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_load_data_success",
              "description": "Tests that load_data correctly parses the file and returns the expected data structure.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "unittest",
        "os",
        "tempfile",
        "unittest.mock",
        "src.data_loader"
      ]
    },
    {
      "path": "tests/test_text_preprocessor.py",
      "language": "python",
      "description": "This file contains unit tests for the text preprocessing components including the TextPreprocessor class, the normalize_text function, and the tokenize_text function. It verifies that diacritics are removed, texts are properly normalized, and tokenization is executed correctly.",
      "classes": [
        {
          "name": "TestTextPreprocessor",
          "description": "Unit tests for text preprocessing functionalities. Tests cover normalization of Arabic text and proper splitting of text tokens.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_preprocess_text_removes_diacritics_and_normalizes",
              "description": "Tests that the TextPreprocessor correctly removes diacritics and normalizes Arabic letters in the input text.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_preprocess_text_no_modification",
              "description": "Tests that the TextPreprocessor leaves already normalized text unchanged.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_arabic_normalization_removes_invisible_and_normalizes",
              "description": "Tests that the normalize_text function removes invisible characters and diacritics while normalizing Arabic letters according to the mapping rules.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_tokenizer_splits_on_punctuation_and_whitespace",
              "description": "Tests that the tokenize_text function correctly splits input text into tokens based on punctuation and whitespace.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/text_preprocessor.py",
        "src/arabic_normalization.py",
        "src/tokenizer.py"
      ]
    },
    {
      "path": "tests/test_logger_config.py",
      "language": "python",
      "description": "Unit tests for the logger configuration ensuring the logger is set up with a FileHandler for the log file.",
      "classes": [
        {
          "name": "TestLoggerConfig",
          "description": "Unit tests for testing logger configuration via the configure_logger function.",
          "parents": [],
          "methods": [
            {
              "name": "test_configure_logger_creates_log_file",
              "description": "Test that the logger is configured with a FileHandler pointing to 'quran_analysis.log'.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/logger_config.py"
      ]
    },
    {
      "path": "tests/test_integration.py",
      "language": "python",
      "description": "Integration tests for the QuranAnalysis application, verifying core functionalities like data processing, log generation, and semantic analysis outputs using the unittest framework.",
      "classes": [
        {
          "name": "TestIntegration",
          "description": "Integration test suite for validating the main application flow and semantic analysis components, extends unittest.TestCase.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_integration_flow",
              "description": "Tests the overall integration flow including data setup, application execution, and log file content verification.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestIntegration",
                  "description": "Instance of TestIntegration."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_semantic_group_cooccurrence_analysis",
              "description": "Tests the semantic group co-occurrence analysis function with sample data and verifies expected output and logging.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestIntegration",
                  "description": "Instance of TestIntegration."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/main.py",
        "src/semantic_analyzer.py"
      ]
    },
    {
      "path": "src/arabic_normalization.py",
      "language": "python",
      "description": "This file implements a function 'normalize_text' that performs comprehensive Arabic text normalization. It removes invisible Unicode characters, strips Arabic diacritics, and maps various Arabic letter forms to their standard forms. Special handling is provided to convert taa marbuta to ha when it follows a ya.",
      "classes": [],
      "functions": [
        {
          "name": "normalize_text",
          "description": "Normalizes Arabic text by removing invisible Unicode characters and diacritics, applying a mapping to standardize Arabic letters, and converting taa marbuta to ha when preceded by ya. The function uses regular expressions for pattern matching and substitution.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input Arabic text that needs normalization."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/tokenizer.py",
      "language": "python",
      "description": "Module to tokenize Arabic text into individual word tokens using whitespace and punctuation as delimiters.",
      "classes": [],
      "functions": [
        {
          "name": "tokenize_text",
          "description": "Tokenize the normalized Arabic text into a list of word tokens based on whitespace and punctuation.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input normalized Arabic text to be tokenized."
            }
          ],
          "return_type": "list"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/lemmatizer.py",
      "language": "python",
      "description": "Module for Arabic lemmatization. Provides functionality to lemmatize Arabic tokens using CAMeL Tools.",
      "classes": [],
      "functions": [
        {
          "name": "lemmatize_token",
          "description": "Lemmatize the given Arabic token using CAMeL Tools. Returns the lemmatized form if successful; otherwise returns the original token.",
          "parameters": [
            {
              "name": "token",
              "type": "string",
              "description": "The Arabic word token."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/root_extractor.py",
      "language": "python",
      "description": "Module for Arabic root word extraction. Provides functionality to extract the root of an Arabic token using CAMeL Tools morphological analysis.",
      "classes": [],
      "functions": [
        {
          "name": "extract_root",
          "description": "Extract the root of the given Arabic token using CAMeL Tools morphological analysis. Returns the root if found; otherwise returns the original token.",
          "parameters": [
            {
              "name": "token",
              "type": "string",
              "description": "The Arabic word token."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "tests/test_lemmatizer.py",
      "language": "Python",
      "description": "Unit tests for the Arabic lemmatizer function. Uses unittest and mock patching to validate the behavior of lemmatize_token by simulating the lemmatizer instance.",
      "classes": [
        {
          "name": "TestLemmatizer",
          "description": "Contains unit tests for the Arabic lemmatizer. Tests the lemmatize_token function to ensure it appends '_lem' to a given token using a mocked lemmatizer instance.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_lemmatize_token",
              "description": "Tests that lemmatize_token returns the token appended with '_lem' using a mocked lemmatizer instance.",
              "parameters": [
                {
                  "name": "mock_lemmatizer_instance",
                  "type": "Mock",
                  "description": "A mocked instance of the internal lemmatizer to simulate the lemmatization behavior."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/lemmatizer"
      ]
    },
    {
      "path": "tests/test_root_extractor.py",
      "language": "Python",
      "description": "Unit tests for the Arabic root extraction functionality. Validates the behavior of extract_root function with both valid analysis and fallback when analysis returns an empty result.",
      "classes": [
        {
          "name": "TestRootExtractor",
          "description": "Contains unit tests for Arabic root extraction, testing both cases when a valid root is returned and when no analysis result is provided.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_extract_root",
              "description": "Tests that extract_root returns the correct root when the analyzer returns a valid result.",
              "parameters": [
                {
                  "name": "mock_analyzer_instance",
                  "type": "Mock",
                  "description": "A mocked analyzer instance to simulate analysis returning a valid root."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_extract_root_no_analysis",
              "description": "Tests that extract_root returns the original token when the analyzer returns an empty result.",
              "parameters": [
                {
                  "name": "mock_analyzer_instance",
                  "type": "Mock",
                  "description": "A mocked analyzer instance to simulate analysis returning an empty list."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/root_extractor"
      ]
    },
    {
      "path": "src/frequency_analyzer.py",
      "language": "python",
      "description": "This module contains multiple functions for performing frequency analyses on Quran text. It includes functions to count word frequencies, analyze word length distribution, compute surah and ayah level frequencies, evaluate root and lemma frequencies, and assess character distributions.",
      "classes": [],
      "functions": [
        {
          "name": "count_word_frequencies",
          "description": "Counts the frequency of each word in the provided tokenized text by iterating over each list of tokens and returning a dictionary of word counts.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists, where each inner list contains words from a verse."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_word_length_distribution",
          "description": "Analyzes the distribution of word lengths in the tokenized text and logs statistics such as total words, average length, and frequency distribution.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists containing tokenized words."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_word_frequency",
          "description": "Analyzes word frequencies at the Surah level by processing Quran data and logging the top 10 frequent words for each Surah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_word_frequency",
          "description": "Analyzes word frequencies at the Ayah level by processing individual verses and logging the top 5 frequent words for each Ayah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_root_word_frequency",
          "description": "Analyzes the frequency of root words across the Quran data by counting occurrences and logging total unique roots and top frequent ones.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data, expected to have a 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_lemma_word_frequency",
          "description": "Analyzes the frequency of lemma words across the Quran data and logs the total unique lemma words along with the top frequent ones.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data, expected to have a 'lemmas' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_root_word_frequency",
          "description": "Analyzes root word frequencies at the Surah level by processing each verse with a text preprocessor and logging the top root words and unique counts for each Surah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_root_word_frequency",
          "description": "Analyzes the frequency of root words for each Ayah by tokenizing the verse and extracting root words, then logging the top 5 roots and unique count.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_first_root_word_frequency",
          "description": "Counts the frequency of the first root word in each Ayah from the Quran data and logs the top 10 frequent first root words.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data, expected to have a 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_last_root_word_frequency",
          "description": "Counts the frequency of the last root word in each Ayah from the Quran data and logs the top 10 frequent last root words.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data, expected to have a 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_semantic_group_frequency",
          "description": "Analyzes the frequency of semantic groups based on root words in the Quran data, logging the total unique groups and top 20 frequent groups.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_character_frequency",
          "description": "Analyzes character frequency in the tokenized text by iterating over each word to count individual characters, logging top characters overall.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists of words from the Quran text."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_character_frequency",
          "description": "Analyzes character frequency at the Surah level by concatenating verse texts and counting character occurrences, logging totals and frequencies.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_character_frequency",
          "description": "Analyzes character frequency at the Ayah level by counting characters in each verse, logging the frequency distribution for each Ayah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/text_preprocessor.py",
        "src/tokenizer.py",
        "src/root_extractor.py"
      ]
    },
    {
      "path": "src/cooccurrence_analyzer.py",
      "language": "python",
      "description": "This module provides functionality to analyze word co-occurrence within each ayah of the Quran data. It tokenizes the text and counts unique co-occurring word pairs, logging the top N pairs and the total number of unique pairs.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_cooccurrence",
          "description": "Analyzes word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_root_word_cooccurrence",
          "description": "Analyzes Root word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique root word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_lemma_word_cooccurrence",
          "description": "Analyzes Lemma word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique lemma word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/tokenizer.py"
      ]
    },
    {
      "path": "src/ngram_analyzer.py",
      "language": "Python",
      "description": "This Python module provides functions for analyzing both word and character n-grams in Quran data at multiple levels (Quran, Surah, and Ayah). It uses sliding window techniques and logging to output frequency analyses.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_ngrams",
          "description": "Analyzes word n-gram frequency for the Quran data by tokenizing each ayah, generating n-grams using a sliding window approach, logging the top frequent n-grams, and returning a Counter object of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of ayahs where each ayah can be a pre-tokenized list or a string that needs to be split."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Size of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "Counter"
        },
        {
          "name": "analyze_surah_word_ngrams",
          "description": "Analyzes word n-gram frequency at the Surah level by grouping Quran data by surah, consolidating tokens, generating n-grams, logging the top 10 frequent n-grams, and returning a dictionary mapping each surah to its Counter of n-grams.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys such as 'surah', 'ayah', 'verse_text', and optionally 'processed_text' and 'surah_name'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Size of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_word_ngrams",
          "description": "Analyzes word n-gram frequency at the Ayah level by processing each ayah, generating n-grams, logging the top 5 n-grams, and returning a dictionary mapping each 'Surah|Ayah' identifier to a Counter object of n-grams.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys like 'surah', 'ayah', 'verse_text', and optionally 'processed_text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Size of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_character_ngrams",
          "description": "Analyzes character n-gram frequency for the entire Quran text by concatenating ayahs, generating character n-grams with a sliding window, logging the top 10 n-grams, and returning a Counter object of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys such as 'processed_text', 'verse_text', or 'text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Length of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "Counter"
        },
        {
          "name": "analyze_surah_character_ngrams",
          "description": "Analyzes character n-grams at the Surah level by grouping Quran data by surah, concatenating texts, generating character n-grams, logging the top 10 n-grams for each surah, and returning a dictionary mapping each surah to its Counter of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys such as 'processed_text', 'verse_text', or 'text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Length of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_character_ngrams",
          "description": "Analyzes character n-grams at the Ayah level for a sample of ayahs (first 5 per surah) by generating n-grams, logging the top 5 n-grams, and returning a dictionary mapping each 'Surah|Ayah' identifier to a Counter object of n-gram frequencies.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with keys like 'surah', 'ayah', and various text fields such as 'processed_text', 'text', or 'verse_text'."
            },
            {
              "name": "n",
              "type": "int",
              "description": "Length of the n-gram (default=2 for bigrams)."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": []
    },
    {
      "path": "tests/test_ngram_analyzer.py",
      "language": "Python",
      "description": "This file contains unit tests for the ngram_analyzer module. It uses Python's unittest framework to validate character n-gram analysis functions at various levels (Quran, Surah, Ayah) and for different n-gram sizes.",
      "classes": [
        {
          "name": "TestCharacterNGrams",
          "description": "Unit tests for verifying character n-gram analysis functions from the ngram_analyzer module. It includes tests for n=2, n=3, and surah/ayah level analyses.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_analyze_character_ngrams_n2",
              "description": "Tests analyze_character_ngrams with n=2 on sample data to ensure correct frequency counts of character bigrams.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_analyze_character_ngrams_n3",
              "description": "Tests analyze_character_ngrams with n=3 on sample data to validate frequency counts of character trigrams.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_analyze_surah_character_ngrams",
              "description": "Tests analyze_surah_character_ngrams by providing sample data and comparing expected Counter outputs for surah-level character n-gram analysis.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_analyze_ayah_character_ngrams",
              "description": "Tests analyze_ayah_character_ngrams to ensure that ayah-level grouping and character n-gram frequency counts match expected results.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/ngram_analyzer.py"
      ]
    },
    {
      "path": "src/collocation_analyzer.py",
      "language": "python",
      "description": "Module for analyzing word collocations in Quran text. Contains a function `analyze_word_collocation` to find and count word pairs within a specified window size in Quranic verses. It preprocesses text, tokenizes it, and uses a sliding window to identify and count collocations, logging the analysis details.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_collocation",
          "description": "Analyze word collocations in the given Quran data using a sliding window.\n    \n    For each ayah in quran_data, the function preprocesses the verse text \n    (using the existing TextPreprocessor if the processed text is not already present)\n    and tokenizes the text into words. For each word, it considers a window of adjacent \n    words (window_size to the left and window_size to the right, excluding the target word)\n    and counts each collocation pair. The collocation pairs are stored in alphabetical order \n    to ensure consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "A list of dictionaries representing ayahs. Each dictionary should contain at least a 'verse_text' key, and may contain a 'processed_text' key."
            },
            {
              "name": "window_size",
              "type": "int",
              "description": "The number of words to consider to the left and right of a target word. Default is 3."
            }
          ],
          "return_type": "Counter"
        }
      ],
      "dependencies": [
        "logging",
        "collections",
        "src.text_preprocessor"
      ]
    },
    {
      "path": "src/semantic_analyzer.py",
      "language": "python",
      "description": "Module for semantic analysis of Quran text. It includes functions to analyze semantic group co-occurrence within Quranic Ayahs by computing unique unordered pairs of semantic groups.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_semantic_group_cooccurrence_ayah",
          "description": "Analyzes semantic group co-occurrence in each Ayah by computing all unordered pairs of semantic groups from the 'semantic_groups' key, logging top pairs and total counts.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data, each expected to contain a 'semantic_groups' key."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/anomaly_detector.py",
      "language": "Python",
      "description": "This file contains functions for anomaly detection in frequency distributions. It computes z-scores for each entry in a distribution and logs anomalies when they exceed a specified threshold. This module is integral to analyzing Quranic feature data for deviations.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_single_distribution",
          "description": "Analyzes a single frequency distribution by computing z-scores and logging anomalies when counts deviate significantly from the mean.",
          "parameters": [
            {
              "name": "feature_name",
              "type": "string",
              "description": "Name of the feature to analyze."
            },
            {
              "name": "distribution",
              "type": "dict",
              "description": "Dictionary mapping feature items to their counts."
            },
            {
              "name": "context",
              "type": "string",
              "description": "Context specifying the level of analysis (e.g., 'Quran', 'Surah', 'Ayah')."
            },
            {
              "name": "threshold",
              "type": "float",
              "description": "Z-score threshold to consider a value anomalous (default is 2.0)."
            }
          ],
          "return_type": "None"
        },
        {
          "name": "analyze_anomaly_detection",
          "description": "Performs anomaly detection across multiple Quranic feature analyses by iterating over provided data and applying the z-score method at different hierarchical levels.",
          "parameters": [
            {
              "name": "analysis_results",
              "type": "dict",
              "description": "Dictionary containing various frequency analysis results for anomaly detection."
            }
          ],
          "return_type": "None"
        }
      ],
      "dependencies": []
    }
  ]
}