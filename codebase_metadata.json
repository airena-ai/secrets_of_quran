{
  "files": [
    {
      "path": "src/__init__.py",
      "language": "python",
      "description": "Initialization script for the src package that enables package-level imports and configurations.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "src/data_loader.py",
      "language": "Python",
      "description": "This file contains the QuranDataLoader class responsible for loading Quran verse data from a text file. It handles parsing lines in the format 'surah|ayah|verse_text' and converts them into a structured list of dictionaries. The class includes error handling for file not found scenarios and proper logging.",
      "classes": [
        {
          "name": "QuranDataLoader",
          "description": "A class that loads and parses Quran data from a text file. Each line in the file is expected to be in the format 'surah|ayah|verse_text'. The class provides functionality to read this data and convert it into a structured format for further processing.",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initializes the QuranDataLoader with an optional file path and sets up logging.",
              "parameters": [
                {
                  "name": "self",
                  "type": "QuranDataLoader",
                  "description": "Instance of the class"
                },
                {
                  "name": "file_path",
                  "type": "str",
                  "description": "Path to the data file. If None, returns empty data."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "load_data",
              "description": "Loads data from the specified file, parsing each line into a dictionary with 'surah', 'ayah', and 'verse_text' keys. Raises FileNotFoundError if the file path is not provided or the file does not exist.",
              "parameters": [
                {
                  "name": "self",
                  "type": "QuranDataLoader",
                  "description": "Instance of the class"
                }
              ],
              "return_type": "list"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "os",
        "logging"
      ]
    },
    {
      "path": "src/text_preprocessor.py",
      "language": "python",
      "description": "Module defining the TextPreprocessor class which processes Arabic text. It performs normalization, tokenization, lemmatization, and root extraction, while logging each step for debugging and verification.",
      "classes": [
        {
          "name": "TextPreprocessor",
          "description": "A class for preprocessing Arabic text by normalizing, tokenizing, lemmatizing, and extracting root words. It logs various processing steps for debugging.",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initialize the TextPreprocessor and configure the logger.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "preprocess_text",
              "description": "Preprocess the Arabic text by performing normalization, tokenization, lemmatization, and root extraction. Returns the processed tokens joined by a space.",
              "parameters": [
                {
                  "name": "text",
                  "type": "string",
                  "description": "The input Arabic text."
                }
              ],
              "return_type": "string"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/arabic_normalization.py",
        "src/tokenizer.py",
        "src/lemmatizer.py",
        "src/root_extractor.py"
      ]
    },
    {
      "path": "src/logger_config.py",
      "language": "python",
      "description": "Configures and returns a logger for the application, logging messages to both console and a log file named 'quran_analysis.log'.",
      "classes": [],
      "functions": [
        {
          "name": "configure_logger",
          "description": "Configures and returns a logger that logs messages to both console and a log file.\n\nThe log file is named 'quran_analysis.log' and is located in the project root.\n\n:return: Configured logger instance.",
          "parameters": [],
          "return_type": "logger"
        }
      ],
      "dependencies": [
        "logging",
        "os"
      ]
    },
    {
      "path": "src/main.py",
      "language": "python",
      "description": "This file serves as the main entry point for the application. It configures logging, loads Quran data, preprocesses text, and performs various analyses including word frequency, co-occurrence, and n-gram analyses. It also handles environment configuration and error logging to ensure a robust and graceful shutdown.",
      "classes": [],
      "functions": [
        {
          "name": "main",
          "description": "Orchestrates data loading, text preprocessing, and multiple analysis steps including frequency analysis, co-occurrence analysis, and n-gram generation. It also handles environment variables and exception logging to manage the overall workflow of the application.",
          "parameters": [],
          "return_type": "None"
        }
      ],
      "dependencies": [
        "src/logger_config.py",
        "src/data_loader.py",
        "src/text_preprocessor.py",
        "src/frequency_analyzer.py",
        "src/cooccurrence_analyzer.py",
        "src/ngram_analyzer.py"
      ]
    },
    {
      "path": "setup.py",
      "language": "python",
      "description": "Setup script for the QuranAnalysis package. Configures package metadata, dependencies, and entry points for console scripts using setuptools.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "requirements.txt",
      "language": "None",
      "description": "Requirements file listing the project's dependencies, including the camel-tools package required for Arabic text processing.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "data/quran-uthmani-min.txt",
      "language": "None",
      "description": "Data file containing the Quran text in a simple format with surah|ayah|verse text.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/__init__.py",
      "language": "python",
      "description": "Initialization script for the tests package that enables discovery and execution of test cases.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/test_data_loader.py",
      "language": "Python",
      "description": "This file contains unit tests for the QuranDataLoader class. It tests the functionality of loading data from a file, including error handling for file not found scenarios. The tests use a temporary file with test data to verify that the loader correctly parses the file and returns the expected data structure.",
      "classes": [
        {
          "name": "TestQuranDataLoader",
          "description": "A test class for the QuranDataLoader that verifies its functionality for loading and parsing Quran data from a text file. It includes tests for both successful data loading and error handling when a file is not found.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "setUp",
              "description": "Sets up the test environment by creating a temporary file with test data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "tearDown",
              "description": "Cleans up the test environment by removing the temporary file.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_load_data_file_not_found",
              "description": "Tests that load_data raises FileNotFoundError when the file doesn't exist.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_load_data_success",
              "description": "Tests that load_data correctly parses the file and returns the expected data structure.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestQuranDataLoader",
                  "description": "Instance of the test class"
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "unittest",
        "os",
        "tempfile",
        "unittest.mock",
        "src.data_loader"
      ]
    },
    {
      "path": "tests/test_text_preprocessor.py",
      "language": "python",
      "description": "This file contains unit tests for the text preprocessing components including the TextPreprocessor class, the normalize_text function, and the tokenize_text function. It verifies that diacritics are removed, texts are properly normalized, and tokenization is executed correctly.",
      "classes": [
        {
          "name": "TestTextPreprocessor",
          "description": "Unit tests for text preprocessing functionalities. Tests cover normalization of Arabic text and proper splitting of text tokens.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_preprocess_text_removes_diacritics_and_normalizes",
              "description": "Tests that the TextPreprocessor correctly removes diacritics and normalizes Arabic letters in the input text.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_preprocess_text_no_modification",
              "description": "Tests that the TextPreprocessor leaves already normalized text unchanged.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_arabic_normalization_removes_invisible_and_normalizes",
              "description": "Tests that the normalize_text function removes invisible characters and diacritics while normalizing Arabic letters according to the mapping rules.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_tokenizer_splits_on_punctuation_and_whitespace",
              "description": "Tests that the tokenize_text function correctly splits input text into tokens based on punctuation and whitespace.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/text_preprocessor.py",
        "src/arabic_normalization.py",
        "src/tokenizer.py"
      ]
    },
    {
      "path": "tests/test_logger_config.py",
      "language": "python",
      "description": "Unit tests for the logger configuration ensuring the logger is set up with a FileHandler for the log file.",
      "classes": [
        {
          "name": "TestLoggerConfig",
          "description": "Unit tests for testing logger configuration via the configure_logger function.",
          "parents": [],
          "methods": [
            {
              "name": "test_configure_logger_creates_log_file",
              "description": "Test that the logger is configured with a FileHandler pointing to 'quran_analysis.log'.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/logger_config.py"
      ]
    },
    {
      "path": "tests/test_integration.py",
      "language": "python",
      "description": "Contains integration tests for the main application workflow. It verifies that the application correctly loads data, processes text, performs various analyses, and logs expected outputs by asserting against log file and file operations.",
      "classes": [
        {
          "name": "TestIntegration",
          "description": "Integration tests for the core user flow ensuring that the main application orchestrates data processing, logging, and file operations correctly.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_integration_flow",
              "description": "Tests the complete integration flow of the main application, including environment setup, sample data creation, invocation of the main function, and verification of expected log messages and file operations.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestIntegration",
                  "description": "Instance of TestIntegration."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/main.py"
      ]
    },
    {
      "path": "src/arabic_normalization.py",
      "language": "python",
      "description": "This file implements a function 'normalize_text' that performs comprehensive Arabic text normalization. It removes invisible Unicode characters, strips Arabic diacritics, and maps various Arabic letter forms to their standard forms. Special handling is provided to convert taa marbuta to ha when it follows a ya.",
      "classes": [],
      "functions": [
        {
          "name": "normalize_text",
          "description": "Normalizes Arabic text by removing invisible Unicode characters and diacritics, applying a mapping to standardize Arabic letters, and converting taa marbuta to ha when preceded by ya. The function uses regular expressions for pattern matching and substitution.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input Arabic text that needs normalization."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/tokenizer.py",
      "language": "python",
      "description": "Module to tokenize Arabic text into individual word tokens using whitespace and punctuation as delimiters.",
      "classes": [],
      "functions": [
        {
          "name": "tokenize_text",
          "description": "Tokenize the normalized Arabic text into a list of word tokens based on whitespace and punctuation.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input normalized Arabic text to be tokenized."
            }
          ],
          "return_type": "list"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/lemmatizer.py",
      "language": "python",
      "description": "Module for Arabic lemmatization. Provides functionality to lemmatize Arabic tokens using CAMeL Tools.",
      "classes": [],
      "functions": [
        {
          "name": "lemmatize_token",
          "description": "Lemmatize the given Arabic token using CAMeL Tools. Returns the lemmatized form if successful; otherwise returns the original token.",
          "parameters": [
            {
              "name": "token",
              "type": "string",
              "description": "The Arabic word token."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/root_extractor.py",
      "language": "python",
      "description": "Module for Arabic root word extraction. Provides functionality to extract the root of an Arabic token using CAMeL Tools morphological analysis.",
      "classes": [],
      "functions": [
        {
          "name": "extract_root",
          "description": "Extract the root of the given Arabic token using CAMeL Tools morphological analysis. Returns the root if found; otherwise returns the original token.",
          "parameters": [
            {
              "name": "token",
              "type": "string",
              "description": "The Arabic word token."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "tests/test_lemmatizer.py",
      "language": "Python",
      "description": "Unit tests for the Arabic lemmatizer function. Uses unittest and mock patching to validate the behavior of lemmatize_token by simulating the lemmatizer instance.",
      "classes": [
        {
          "name": "TestLemmatizer",
          "description": "Contains unit tests for the Arabic lemmatizer. Tests the lemmatize_token function to ensure it appends '_lem' to a given token using a mocked lemmatizer instance.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_lemmatize_token",
              "description": "Tests that lemmatize_token returns the token appended with '_lem' using a mocked lemmatizer instance.",
              "parameters": [
                {
                  "name": "mock_lemmatizer_instance",
                  "type": "Mock",
                  "description": "A mocked instance of the internal lemmatizer to simulate the lemmatization behavior."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/lemmatizer"
      ]
    },
    {
      "path": "tests/test_root_extractor.py",
      "language": "Python",
      "description": "Unit tests for the Arabic root extraction functionality. Validates the behavior of extract_root function with both valid analysis and fallback when analysis returns an empty result.",
      "classes": [
        {
          "name": "TestRootExtractor",
          "description": "Contains unit tests for Arabic root extraction, testing both cases when a valid root is returned and when no analysis result is provided.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_extract_root",
              "description": "Tests that extract_root returns the correct root when the analyzer returns a valid result.",
              "parameters": [
                {
                  "name": "mock_analyzer_instance",
                  "type": "Mock",
                  "description": "A mocked analyzer instance to simulate analysis returning a valid root."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_extract_root_no_analysis",
              "description": "Tests that extract_root returns the original token when the analyzer returns an empty result.",
              "parameters": [
                {
                  "name": "mock_analyzer_instance",
                  "type": "Mock",
                  "description": "A mocked analyzer instance to simulate analysis returning an empty list."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/root_extractor"
      ]
    },
    {
      "path": "src/frequency_analyzer.py",
      "language": "python",
      "description": "This module provides a collection of functions to perform various analyses on Quran text data, including word frequency, word length distribution, character frequency, and root/lemma analyses at both Surah and Ayah levels.",
      "classes": [],
      "functions": [
        {
          "name": "count_word_frequencies",
          "description": "Count the frequency of each word in the tokenized text.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists, where each inner list contains words from a verse."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_word_length_distribution",
          "description": "Analyze the distribution of word lengths in the tokenized text and log summary statistics.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of lists containing tokenized words."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_word_frequency",
          "description": "Analyze word frequencies at the Surah level and log the top 10 frequent words for each Surah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_word_frequency",
          "description": "Analyze word frequencies at the Ayah level and log the top 5 frequent words for each Ayah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_root_word_frequency",
          "description": "Analyze the frequency of root words across the Quran data and log the total unique count and top 1000 frequent roots.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with 'roots' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_lemma_word_frequency",
          "description": "Analyze the frequency of lemma words across the Quran data and log the total unique count and top 1000 frequent lemmas.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with 'lemmas' key."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_root_word_frequency",
          "description": "Analyze the frequency of root words at the Surah level and log the top 10 frequent roots and unique count.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_root_word_frequency",
          "description": "Analyze the frequency of root words at the Ayah level and log the top 5 frequent roots and unique count for each Ayah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries containing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_character_frequency",
          "description": "Analyze the frequency of each character in the tokenized text and log the top 20 most frequent characters.",
          "parameters": [
            {
              "name": "tokenized_text",
              "type": "list",
              "description": "List of tokenized ayahs, where each ayah is a list of words."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_surah_character_frequency",
          "description": "Analyze character frequency at the Surah level by concatenating ayahs and logging the total character count and frequencies.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_character_frequency",
          "description": "Analyze character frequency at the Ayah level and log the character count and frequencies for each ayah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/text_preprocessor.py",
        "src/tokenizer.py",
        "src/root_extractor.py"
      ]
    },
    {
      "path": "src/cooccurrence_analyzer.py",
      "language": "python",
      "description": "This module provides functionality to analyze word co-occurrence within each ayah of the Quran data. It tokenizes the text and counts unique co-occurring word pairs, logging the top N pairs and the total number of unique pairs.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_cooccurrence",
          "description": "Analyzes word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_root_word_cooccurrence",
          "description": "Analyzes Root word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique root word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_lemma_word_cooccurrence",
          "description": "Analyzes Lemma word co-occurrence within each ayah of the Quran data. It tokenizes the verse text and counts unique lemma word pairs, ensuring pairs are stored in alphabetical order for consistency.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries, each containing 'surah', 'ayah', and either 'processed_text' or 'verse_text'."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": [
        "src/tokenizer.py"
      ]
    },
    {
      "path": "src/ngram_analyzer.py",
      "language": "python",
      "description": "Provides a collection of functions for performing n-gram analysis on Quran data at both the word and character levels. This module supports analysis on the entire text as well as segmented analyses at the Surah and Ayah levels, logging detailed frequency counts and top n-grams.",
      "classes": [],
      "functions": [
        {
          "name": "analyze_word_ngrams",
          "description": "Analyzes word n-gram frequency by tokenizing ayahs and counting consecutive word sequences using a sliding window. Logs the top 20 most frequent n-grams and returns a Counter object.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of ayahs, where each ayah is either a pre-tokenized list or a string that requires splitting."
            },
            {
              "name": "n",
              "type": "int",
              "description": "The size of the n-gram to generate, default is 2."
            }
          ],
          "return_type": "Counter"
        },
        {
          "name": "analyze_surah_word_ngrams",
          "description": "Analyzes word n-gram frequency at the Surah level by grouping ayahs per Surah, tokenizing text, and counting n-grams using a sliding window approach. Logs the top 20 n-grams and unique n-gram counts for each Surah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data for each ayah."
            },
            {
              "name": "n",
              "type": "int",
              "description": "The n-gram size, default is 2."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_word_ngrams",
          "description": "Analyzes word n-gram frequency at the Ayah level by tokenizing each ayah's text and counting consecutive word groups. Logs the top 20 n-grams along with the total unique n-grams for each Ayah.",
          "parameters": [
            {
              "name": "data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with ayah details."
            },
            {
              "name": "n",
              "type": "int",
              "description": "The n-gram size, default is 2."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_character_ngrams",
          "description": "Analyzes character n-gram frequency over the entire Quran text by concatenating ayahs and sliding a window across characters. Logs the top 10 most frequent n-grams and returns a Counter object.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data with text content."
            },
            {
              "name": "n",
              "type": "int",
              "description": "The length of the n-gram, default is 2."
            }
          ],
          "return_type": "Counter"
        },
        {
          "name": "analyze_surah_character_ngrams",
          "description": "Analyzes character n-gram frequency at the Surah level by aggregating text per Surah and counting n-grams using a sliding window. Logs the top 5 n-grams and total unique n-gram counts for each Surah.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries representing Quran data."
            },
            {
              "name": "n",
              "type": "int",
              "description": "The length of the character n-gram, default is 2."
            }
          ],
          "return_type": "dict"
        },
        {
          "name": "analyze_ayah_character_ngrams",
          "description": "Analyzes character n-gram frequency at the Ayah level by processing each ayah's text and counting n-grams. Logs the top 3 n-grams along with the frequency count and returns the results as a dictionary.",
          "parameters": [
            {
              "name": "quran_data",
              "type": "list",
              "description": "List of dictionaries containing Quran data with text information for each ayah."
            },
            {
              "name": "n",
              "type": "int",
              "description": "The length of the character n-gram, default is 2."
            }
          ],
          "return_type": "dict"
        }
      ],
      "dependencies": []
    },
    {
      "path": "tests/test_ngram_analyzer.py",
      "language": "python",
      "description": "Unit tests for the n-gram analyzer module, verifying the correctness of character n-gram analysis functions at the Quran, Surah, and Ayah levels.",
      "classes": [
        {
          "name": "TestCharacterNGrams",
          "description": "Unit tests for n-gram analysis functions in the ngram_analyzer module to ensure accurate frequency counts of character n-grams.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_analyze_character_ngrams_n2",
              "description": "Tests the analyze_character_ngrams function with n=2 to verify correct bigram frequency counting on sample data.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestCharacterNGrams",
                  "description": "Instance of TestCharacterNGrams."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_character_ngrams_n3",
              "description": "Tests the analyze_character_ngrams function with n=3 to verify correct trigram frequency counting on sample input.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestCharacterNGrams",
                  "description": "Instance of TestCharacterNGrams."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_surah_character_ngrams",
              "description": "Tests the analyze_surah_character_ngrams function to ensure it correctly aggregates and counts character n-grams at the Surah level from multiple entries.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestCharacterNGrams",
                  "description": "Instance of TestCharacterNGrams."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "test_analyze_ayah_character_ngrams",
              "description": "Tests the analyze_ayah_character_ngrams function to verify correct grouping and frequency counting of character n-grams at the Ayah level.",
              "parameters": [
                {
                  "name": "self",
                  "type": "TestCharacterNGrams",
                  "description": "Instance of TestCharacterNGrams."
                }
              ],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/ngram_analyzer.py"
      ]
    }
  ]
}