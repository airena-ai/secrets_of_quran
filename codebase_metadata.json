{
  "files": [
    {
      "path": "src/__init__.py",
      "language": "python",
      "description": "Initialization script for the src package that enables package-level imports and configurations.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "src/data_loader.py",
      "language": "python",
      "description": "Module that defines the QuranDataLoader class to load and parse Quran text data from a file based on expected format, and logs each step of the data loading process.",
      "classes": [
        {
          "name": "QuranDataLoader",
          "description": "A class to load and parse Quran text data from a file. Uses a default file path when none is provided.",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initialize the QuranDataLoader with an optional file path. If none is provided, a default data file is used.",
              "parameters": [
                {
                  "name": "file_path",
                  "type": "string",
                  "description": "Optional path to the Quran text file. Defaults to a preset file if None."
                }
              ],
              "return_type": "None"
            },
            {
              "name": "load_data",
              "description": "Load the Quran data from the text file, parsing each non-empty line into surah, ayah, and verse_text, and logging relevant information.",
              "parameters": [],
              "return_type": "list"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "src/text_preprocessor.py",
      "language": "python",
      "description": "Module that defines the TextPreprocessor class to normalize and tokenize Arabic text, including logging of processing steps.",
      "classes": [
        {
          "name": "TextPreprocessor",
          "description": "A class for preprocessing Arabic text that initializes a logger and provides methods to normalize and tokenize Arabic text.",
          "parents": [],
          "methods": [
            {
              "name": "__init__",
              "description": "Initialize the TextPreprocessor and set up the logger.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "preprocess_text",
              "description": "Preprocess the given Arabic text by normalizing and tokenizing the text while logging processing details.",
              "parameters": [
                {
                  "name": "text",
                  "type": "string",
                  "description": "The input Arabic text to be processed."
                }
              ],
              "return_type": "string"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/arabic_normalization.py",
        "src/tokenizer.py"
      ]
    },
    {
      "path": "src/logger_config.py",
      "language": "python",
      "description": "Module that configures and sets up the logging for the application, writing log messages to 'quran_analysis.log' with a predefined format.",
      "classes": [],
      "functions": [
        {
          "name": "configure_logger",
          "description": "Configure the root logger to write log messages to 'quran_analysis.log' and set logging level to INFO. It clears existing handlers before attaching a new FileHandler.",
          "parameters": [],
          "return_type": "logging.Logger"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/main.py",
      "language": "python",
      "description": "Main module that initiates the application. It sets up logging, data loading, and text preprocessing, and ensures proper resource cleanup.",
      "classes": [],
      "functions": [
        {
          "name": "main",
          "description": "Main function to orchestrate data loading and text preprocessing. It configures the logger, reads the data file path from the environment variable, loads data using QuranDataLoader, processes text using TextPreprocessor, and handles cleanup of log file handlers.",
          "parameters": [],
          "return_type": "None"
        }
      ],
      "dependencies": [
        "src/logger_config.py",
        "src/data_loader.py",
        "src/text_preprocessor.py"
      ]
    },
    {
      "path": "setup.py",
      "language": "python",
      "description": "Setup script for packaging and distribution of the quran_text_analysis application. It defines package metadata, dependencies, and console entry points.",
      "classes": [],
      "functions": [],
      "dependencies": [
        "src/main.py"
      ]
    },
    {
      "path": "requirements.txt",
      "language": "None",
      "description": "Lists required Python packages for the project. Currently empty indicating no external dependencies.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "data/quran-uthmani-min.txt",
      "language": "None",
      "description": "Data file containing the Quran text in a simple format with surah|ayah|verse text.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/__init__.py",
      "language": "python",
      "description": "Initialization script for the tests package that enables discovery and execution of test cases.",
      "classes": [],
      "functions": [],
      "dependencies": []
    },
    {
      "path": "tests/test_data_loader.py",
      "language": "python",
      "description": "Unit tests for the QuranDataLoader class, testing data loading and error handling scenarios.",
      "classes": [
        {
          "name": "TestQuranDataLoader",
          "description": "Unit tests for the QuranDataLoader class methods.",
          "parents": [],
          "methods": [
            {
              "name": "test_load_data_success",
              "description": "Test that loading data from a valid file returns a correctly parsed list of dictionaries.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_load_data_file_not_found",
              "description": "Test that a non-existent file raises a FileNotFoundError.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/data_loader.py"
      ]
    },
    {
      "path": "tests/test_text_preprocessor.py",
      "language": "python",
      "description": "This file contains unit tests for the text preprocessing components including the TextPreprocessor class, the normalize_text function, and the tokenize_text function. It verifies that diacritics are removed, texts are properly normalized, and tokenization is executed correctly.",
      "classes": [
        {
          "name": "TestTextPreprocessor",
          "description": "Unit tests for text preprocessing functionalities. Tests cover normalization of Arabic text and proper splitting of text tokens.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_preprocess_text_removes_diacritics_and_normalizes",
              "description": "Tests that the TextPreprocessor correctly removes diacritics and normalizes Arabic letters in the input text.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_preprocess_text_no_modification",
              "description": "Tests that the TextPreprocessor leaves already normalized text unchanged.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_arabic_normalization_removes_invisible_and_normalizes",
              "description": "Tests that the normalize_text function removes invisible characters and diacritics while normalizing Arabic letters according to the mapping rules.",
              "parameters": [],
              "return_type": "None"
            },
            {
              "name": "test_tokenizer_splits_on_punctuation_and_whitespace",
              "description": "Tests that the tokenize_text function correctly splits input text into tokens based on punctuation and whitespace.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/text_preprocessor.py",
        "src/arabic_normalization.py",
        "src/tokenizer.py"
      ]
    },
    {
      "path": "tests/test_logger_config.py",
      "language": "python",
      "description": "Unit tests for the logger configuration ensuring the logger is set up with a FileHandler for the log file.",
      "classes": [
        {
          "name": "TestLoggerConfig",
          "description": "Unit tests for testing logger configuration via the configure_logger function.",
          "parents": [],
          "methods": [
            {
              "name": "test_configure_logger_creates_log_file",
              "description": "Test that the logger is configured with a FileHandler pointing to 'quran_analysis.log'.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/logger_config.py"
      ]
    },
    {
      "path": "tests/test_integration.py",
      "language": "python",
      "description": "Integration tests that exercise the core user flow of the application, including data loading, logging, and overall execution of the main function.",
      "classes": [
        {
          "name": "TestIntegration",
          "description": "Integration tests for verifying the complete application flow including data file handling and log file creation.",
          "parents": [
            "unittest.TestCase"
          ],
          "methods": [
            {
              "name": "test_integration_flow",
              "description": "Tests the integration flow by invoking the main function, verifying log file creation and checking expected log messages in the log file.",
              "parameters": [],
              "return_type": "None"
            }
          ]
        }
      ],
      "functions": [],
      "dependencies": [
        "src/main.py"
      ]
    },
    {
      "path": "src/arabic_normalization.py",
      "language": "python",
      "description": "This file implements a function 'normalize_text' that performs comprehensive Arabic text normalization. It removes invisible Unicode characters, strips Arabic diacritics, and maps various Arabic letter forms to their standard forms. Special handling is provided to convert taa marbuta to ha when it follows a ya.",
      "classes": [],
      "functions": [
        {
          "name": "normalize_text",
          "description": "Normalizes Arabic text by removing invisible Unicode characters and diacritics, applying a mapping to standardize Arabic letters, and converting taa marbuta to ha when preceded by ya. The function uses regular expressions for pattern matching and substitution.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input Arabic text that needs normalization."
            }
          ],
          "return_type": "string"
        }
      ],
      "dependencies": []
    },
    {
      "path": "src/tokenizer.py",
      "language": "python",
      "description": "Module to tokenize Arabic text into individual word tokens using whitespace and punctuation as delimiters.",
      "classes": [],
      "functions": [
        {
          "name": "tokenize_text",
          "description": "Tokenize the normalized Arabic text into a list of word tokens based on whitespace and punctuation.",
          "parameters": [
            {
              "name": "text",
              "type": "string",
              "description": "The input normalized Arabic text to be tokenized."
            }
          ],
          "return_type": "list"
        }
      ],
      "dependencies": []
    }
  ]
}